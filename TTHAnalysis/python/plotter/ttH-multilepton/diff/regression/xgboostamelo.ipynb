{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n",
      "You are running on  wudangshan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from train_utils import *\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "import uproot\n",
    "from root_pandas import read_root\n",
    "\n",
    "\n",
    "import socket\n",
    "my_hostname=socket.gethostbyaddr(socket.gethostname())[0]\n",
    "print('You are running on ', my_hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "def load_data(debug=False):\n",
    "    idir = '{}'.format('/nfs/user/pvischia/tth/dnn/' if 'cism.ucl.be' in my_hostname else './')\n",
    "    file_tth = \"{}/tree_2lss1tau.root\".format(idir)\n",
    "    key = \"Friends\"\n",
    "    data_tth_orig = read_root(file_tth, key)\n",
    "    if debug:\n",
    "        data_tth_orig.describe()\n",
    "    return data_tth_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-jag the data with respect to the jets\n",
    "\n",
    "def dejag_data(data, debug=False):\n",
    "    test=data['SelJet_pt']\n",
    "    l = [len(i) for i in test]\n",
    "    maxl = max(l)\n",
    "\n",
    "    for lab in ['pt', 'eta', 'phi', 'mass', 'isBtag', 'isFromHadTop', 'btagDeepFlavB']:\n",
    "        label='SelJet_%s'%lab\n",
    "        tempLab=data[label]\n",
    "        data.drop([label], axis=1, inplace=True)\n",
    "        out = pd.DataFrame(tempLab.tolist(),columns=[ 'SelJet%s_%s'%(i, lab) for i in range(maxl)])\n",
    "        data = pd.concat([data, out], axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ill-defined mass columns\n",
    "def remove_masses(data, debug=False):\n",
    "    for lab in data.columns:\n",
    "        if 'mass' in lab:\n",
    "            data.drop([lab], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_names(useJets=0, debug=False):\n",
    "    thevars=[\n",
    "            'Lep1_pt', \n",
    "            'Lep2_pt', 'Lep1_eta', 'Lep2_eta', 'Lep1_phi', 'Lep2_phi',\n",
    "             'nSelJets',\n",
    "             'met', 'met_phi', \n",
    "             'HTT_score', \n",
    "             'Hj_tagger_hadTop',\n",
    "             'mindr_lep2_jet', 'mindr_lep1_jet', 'avg_dr_jet',\n",
    "             'dPhiLL_BBframe_2lss', 'dEtaLL_BBframe_2lss', 'dPhiBB_LLframe_2lss',\n",
    "             'dEtaBB_LLframe_2lss',\n",
    "             'dEtaBB_2lss',\n",
    "             'mTTH_2lss1tau',\n",
    "             'theta_higgs_ttbar_TTHsystem_2lss1tau',\n",
    "             'thetaTopTop_ttbarframe_2lss1tau', \n",
    "             'Tau_pt', 'Tau_eta', 'Tau_phi'\n",
    "            ]\n",
    "    if useJets>0:\n",
    "        for i in range(useJets):\n",
    "            thevars.append('SelJet%s_pt'%i)\n",
    "            thevars.append('SelJet%s_eta'%i)\n",
    "            thevars.append('SelJet%s_phi'%i)\n",
    "            #thevars.append('SelJet%s_mass'%i)\n",
    "            thevars.append('SelJet%s_isBtag'%i)\n",
    "            thevars.append('SelJet%s_isFromHadTop'%i)\n",
    "            thevars.append('SelJet%s_btagDeepFlavB'%i)\n",
    "    if debug:\n",
    "        print('Training features:', thevars)\n",
    "        \n",
    "    return thevars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(data, features, oddevensplit=False, fillna=False, debug=False):\n",
    "    labs=deepcopy(features)\n",
    "    if oddevensplit:\n",
    "        labs.append('event')\n",
    "    bkg = data[labs]\n",
    "    sig = data[labs]\n",
    "\n",
    "    if fillna:\n",
    "        bkg.fillna(0, inplace=True)\n",
    "        sig.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    wgtbkg=pd.DataFrame(data['weight_SM'].tolist(), columns=['weight'])\n",
    "    wgtsig=pd.DataFrame(data['weight_CP_odd'].tolist(), columns=['weight'])\n",
    "\n",
    "    labbkg = pd.DataFrame(np.zeros_like(bkg[features[0]]).tolist(), columns=['label'])\n",
    "    labsig = pd.DataFrame(np.ones_like(bkg[features[0]]).tolist(), columns=['label'])\n",
    "\n",
    "    bkg = pd.concat([bkg, wgtbkg, labbkg], axis=1)\n",
    "    sig = pd.concat([sig, wgtsig, labsig], axis=1)\n",
    "\n",
    "    sig['weight'] = sig['weight'].apply(lambda x: x if x <10. else 10.)\n",
    "\n",
    "    labelled_set=pd.concat([sig, bkg], axis=0)\n",
    "    if False:\n",
    "        sns.pairplot(labelled_set, hue='label')\n",
    "    if oddevensplit:\n",
    "        labs.append('weight')\n",
    "        X=labelled_set[labs]\n",
    "        y=labelled_set['label']\n",
    "        weight=labelled_set['weight']\n",
    "        if debug:\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            print(bkg.head())\n",
    "            print(sig.head())\n",
    "\n",
    "        X_wgt=X['weight']\n",
    "\n",
    "        X_train=X[X['event']%2 == 0]\n",
    "        X_test =X[X['event']%2 != 0]\n",
    "        y_train=y[X['event']%2 == 0]\n",
    "        y_test =y[X['event']%2 != 0]\n",
    "        \n",
    "        X_train_wgt=X_train['weight']\n",
    "        X_train=X_train.drop(['weight'], axis=1, inplace=False)\n",
    "        X_train=X_train.drop(['event'], axis=1, inplace=False)\n",
    "\n",
    "        X_test_wgt=X_test['weight']\n",
    "        X_test=X_test.drop(['weight'], axis=1, inplace=False)\n",
    "        X_test=X_test.drop(['event'], axis=1, inplace=False)\n",
    "\n",
    "        return {'X_train'     : X_train,\n",
    "                'X_test'      : X_test, \n",
    "                'X_train_wgt' : X_train_wgt,\n",
    "                'X_test_wgt'  : X_test_wgt,\n",
    "                'y_train'     : y_train,\n",
    "                'y_test'      : y_test \n",
    "               }\n",
    "    else:\n",
    "        labelled_set=shuffle(labelled_set)\n",
    "        labs.append('weight')\n",
    "        X=labelled_set[labs]\n",
    "        y=labelled_set['label']\n",
    "        weight=labelled_set['weight']\n",
    "        if debug:\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            print(bkg.head())\n",
    "            print(sig.head())\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1534534, shuffle=True)\n",
    "\n",
    "        X_train_wgt=X_train['weight']\n",
    "        X_train=X_train.drop(['weight'], axis=1, inplace=False)\n",
    "        X_test_wgt=X_test['weight']\n",
    "        X_test=X_test.drop(['weight'], axis=1, inplace=False)\n",
    "\n",
    "        return {'X_train'     : X_train,\n",
    "                'X_test'      : X_test, \n",
    "                'X_train_wgt' : X_train_wgt,\n",
    "                'X_test_wgt'  : X_test_wgt,\n",
    "                'y_train'     : y_train,\n",
    "                'y_test'      : y_test\n",
    "               }\n",
    "        #train_dmatrix = xgb.DMatrix(data=X_train,label=y_train,weight=X_train_wgt)\n",
    "        #test_dmatrix = xgb.DMatrix(data=X_test,label=y_test,weight=X_test_wgt)\n",
    "\n",
    "    #perm = np.random.permutation(len(X_train))\n",
    "    #X_train = X_train[perm]\n",
    "    #y_train = y_train[perm]\n",
    "    #X_train_wgt = X_train_wgt[perm]\n",
    "\n",
    "    #perm = np.random.permutation(len(X_train))\n",
    "    #X_test = X_test[perm]\n",
    "    #y_test = y_test[perm]\n",
    "    #X_test_wgt = X_test_wgt[perm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(X, y, wgt, features, label):\n",
    "    fix, axs= plt.subplots(10,10, figsize=(40,40))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i,ax in enumerate(axs):\n",
    "        if i >= len(features):\n",
    "            continue\n",
    "        var=features[i]\n",
    "        if var=='weight':\n",
    "            continue\n",
    "        ax=axs[i]\n",
    "        #ax.hist(X_train[var], label='%s'%var)\n",
    "        ax.hist(X[var][y[:]==0 ], weights=wgt[y[:]==0 ], density=True, alpha=0.5, bins=20)\n",
    "        ax.hist(X[var][y[:]==1 ], weights=wgt[y[:]==1 ], density=True, alpha=0.5, bins=20)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_title('%s dataset'%label)\n",
    "        ax.set_xlabel(var)\n",
    "    plt.show()\n",
    "    plt.hist(wgt[y[:]==0 ], alpha=0.5, bins=np.linspace(0,20,200))\n",
    "    plt.yscale(\"log\")\n",
    "    plt.hist(wgt[y[:]==1 ], alpha=0.5, bins=np.linspace(0,20,200))\n",
    "    plt.title('%s dataset')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalized=True, cmap='bone'):\n",
    "    plt.figure(figsize=[7, 6])\n",
    "    norm_cm = cm\n",
    "    if normalized:\n",
    "        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "debug=False\n",
    "data_tth_orig = load_data(debug=debug)\n",
    "data_tth=data_tth_orig # save the original\n",
    "data_tth=dejag_data(data_tth, debug=debug)\n",
    "data_tth=remove_masses(data_tth, debug=debug)\n",
    "    \n",
    "the_initial_vars=load_feature_names(useJets=0, debug=debug) # add \"useJets\" quadrimomenta of jets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single model (the basic template)\n",
    "\n",
    "def train_single_model(params, data, features, oddevensplit=True, fillna=False, debug=False):\n",
    "    # Start with a first thing with all variables\n",
    "    datasets= get_train_and_test(data, features, oddevensplit=oddevensplit, fillna=fillna, debug=debug)\n",
    "    X_train     = datasets['X_train']\n",
    "    X_test      = datasets['X_test']\n",
    "    X_train_wgt = datasets['X_train_wgt']\n",
    "    X_test_wgt  = datasets['X_test_wgt']\n",
    "    y_train     = datasets['y_train']\n",
    "    y_test      = datasets['y_test']\n",
    "\n",
    "    if debug:\n",
    "        plot_features(X_train, y_train, X_train_wgt, the_initial_vars, 'training')\n",
    "        plot_features(X_test , y_test , X_test_wgt , the_initial_vars, 'test')\n",
    "    # WORKING PROTOTYPE: xg_class = xgb.XGBClassifier(n_estimators=120, max_depth=3, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, gamma=1)\n",
    "    # Theoretically it should just accept \"params=params\"\n",
    "    xg_class = xgb.XGBClassifier(n_estimators=params['n_estimators'], \n",
    "                                 max_depth=params['max_depth'],\n",
    "                                 learning_rate=params['learning_rate'],\n",
    "                                 subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],\n",
    "                                 gamma=params['gamma'],\n",
    "                                 silent=True\n",
    "                                )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xg_class.fit(X=X_train,y=y_train,eval_metric=[\"error\", \"logloss\"], silent=True, early_stopping_rounds=10, eval_set=eval_set,sample_weight=X_train_wgt,sample_weight_eval_set=[X_train_wgt, X_test_wgt])\n",
    "\n",
    "    preds_train = xg_class.predict_proba(X_train)[:,0]\n",
    "    preds_test = xg_class.predict_proba(X_test)[:,0]\n",
    "\n",
    "    #preds_train=np.argmax(xg_class.predict_proba(X_train), axis=1)\n",
    "    #preds_test =np.argmax(xg_class.predict_proba(X_test), axis=1)\n",
    "\n",
    "    preds_cat_train = xg_class.predict(X_train)\n",
    "    preds_cat_test  = xg_class.predict(X_test)\n",
    "    # ROC curves\n",
    "    auc_train=plot_roc(y_train, preds_train, sample_weight=X_train_wgt, label='training', plot=False, debug=False)\n",
    "    auc_test=plot_roc(y_test, preds_test, sample_weight=X_test_wgt, label='test', plot=False, debug=False)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, preds_cat_train,sample_weight=X_train_wgt)\n",
    "    print(\"Accuracy (train): %.2f%%\" % (accuracy_train * 100.0))\n",
    "    accuracy_test = accuracy_score(y_test, preds_cat_test, sample_weight=X_test_wgt)\n",
    "    print(\"Accuracy (test): %.2f%%\" % (accuracy_test * 100.0))\n",
    "\n",
    "    if debug:\n",
    "        xgb.plot_importance(xg_class)\n",
    "        plt.figure(figsize = (16, 12))\n",
    "        plt.show()        \n",
    "        results = xg_class.evals_result()\n",
    "        epochs = len(results['validation_0']['logloss'])\n",
    "        x_axis = range(0, epochs)\n",
    "        # plot log loss\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.title('XGBoost Log Loss')\n",
    "        plt.show()\n",
    "        # plot classification error\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('Classification Error')\n",
    "        plt.title('XGBoost Classification Error')\n",
    "        plt.show()\n",
    "\n",
    "        cm = confusion_matrix(y_test, preds_cat_test, sample_weight=X_test_wgt)\n",
    "        plot_confusion_matrix(cm, ['CP odd', 'CP even'])\n",
    "        plot_rel_pred(y_test, preds_test, 'test')\n",
    "        plot_pred(y_test, preds_test, 'test')\n",
    "        plot_weights(X_test_wgt, y_test, 'test')\n",
    "        \n",
    "    return {'auc_train': auc_train, 'auc_test': auc_test, 'model': xg_class, 'features': features, 'sorted_importance': dict(sorted(xg_class.get_booster().get_score(importance_type='weight').items(), key=lambda item: item[1])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED (it claims XGBClassifier doesn't implement the fit() method, which is manifestly untrue)\n",
    "def run_optimization_randomizedsearch(data, features, oddevensplit=True, fillna=False, debug=False):\n",
    "    # Here must pass the parameters dictionary params, etc\n",
    "    # (must change train_single_model to accept the dictionary of parameters,\n",
    "    # and then set up the full grid scan)\n",
    "    #train_res= train_single_model(data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "\n",
    "    datasets= get_train_and_test(data, features, oddevensplit=oddevensplit, fillna=fillna, debug=debug)\n",
    "    X_train     = datasets['X_train']\n",
    "    X_test      = datasets['X_test']\n",
    "    X_train_wgt = datasets['X_train_wgt']\n",
    "    X_test_wgt  = datasets['X_test_wgt']\n",
    "    y_train     = datasets['y_train']\n",
    "    y_test      = datasets['y_test']\n",
    "\n",
    "    grid={'learning_rate' : [0.01, 0.02, 0.05, 0.1],\n",
    "          'n_estimators' : [50, 100, 200, 500, 1000],\n",
    "          'max_depth' : [3, 4, 5, 8],\n",
    "          'subsample' : [0.5, 0.8, 0.9, 1.0],\n",
    "          'colsample_bytree' : [0.3, 0.5, 0.8, 1.0],\n",
    "          'gamma' : [0, 1, 5]\n",
    "         }\n",
    "    list(ParameterGrid(grid))   \n",
    "    xg_class = xgb.XGBClassifier(n_estimators=params['n_estimators'], \n",
    "                                 max_depth=params['max_depth'],\n",
    "                                 learning_rate=params['learning_rate'],\n",
    "                                 subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],\n",
    "                                 gamma=params['gamma'])\n",
    "    \n",
    "    folds = 3\n",
    "    param_comb = 5\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "    random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    random_search.fit(X=X_train,y=y_train,eval_metric=[\"error\", \"logloss\"], early_stopping_rounds=10, eval_set=eval_set,sample_weight=X_train_wgt,sample_weight_eval_set=[X_train_wgt, X_test_wgt])\n",
    "    print('\\n Best estimator:')\n",
    "    print(random_search.best_estimator_)\n",
    "    print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "    print(random_search.best_score_ * 2 - 1)\n",
    "    print('\\n Best hyperparameters:')\n",
    "    print(random_search.best_params_)\n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full hyperparameters optimization for a single set of variables\n",
    "\n",
    "# I know I should use e.g. Parzen tree estimators from hyperopt as I did for the charged Higgs with 2016 dataset,\n",
    "# but I feel lazy and the hyperparameter space I want to check is actually not that large,\n",
    "# so I am implementing a simple grid search\n",
    "\n",
    "def run_optimization(data, features, oddevensplit=True, fillna=False, debug=False):\n",
    "    # Here must pass the parameters dictionary params, etc\n",
    "    # (must change train_single_model to accept the dictionary of parameters,\n",
    "    # and then set up the full grid scan)\n",
    "    #train_res= train_single_model(data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "\n",
    "    grid={'learning_rate' : [0.01, 0.02, 0.05, 0.1],\n",
    "          'n_estimators' : [50, 100, 200, 500, 1000],\n",
    "          'max_depth' : [3, 4, 5, 8],\n",
    "          'subsample' : [0.5, 0.8, 0.9, 1.0],\n",
    "          'colsample_bytree' : [0.3, 0.5, 0.8, 1.0],\n",
    "          'gamma' : [0, 1, 5]\n",
    "         }\n",
    "       \n",
    "    listgrid=list(ParameterGrid(grid))   \n",
    "    gridresults=[]\n",
    "    for params in listgrid:\n",
    "        train_res= train_single_model(params, data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "        gridresults.append([train_res, train_res['auc_test']])\n",
    "        print('MAXIMUM HAS NOW AUC', max(gridresults,key=lambda item:item[1])[1])\n",
    "        print('FULL LIST IS', gridresults)\n",
    "    return(max(gridresults,key=lambda item:item[1])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a single model\n",
    "if False:\n",
    "    params={'learning_rate' : 0.1,\n",
    "          'n_estimators' : 120,\n",
    "          'max_depth' : 4,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree' : 1,\n",
    "          'gamma' : 1\n",
    "         }\n",
    "    train_res= train_single_model(params, data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# Full  optimization, where \"best\" is w.r.t. the AUC metric:\n",
    "# 1. start with a variable set\n",
    "# 2. find the model with the best hyperparameter values for that set of variables\n",
    "# 3. remove the variable that is the least important in this model\n",
    "# 4. return to point 2\n",
    "# This ends when removing variables doesnt' improve the AUC anymore\n",
    "\n",
    "\n",
    "# So far for each variables set I run a single model.\n",
    "# When I'll have implemented run_optimization (for a given features set) I'll substitute a call to it here\n",
    "# in place of the call to train_single_model\n",
    "auc_per_var=[]\n",
    "features_set=deepcopy(the_initial_vars)\n",
    "for i in range(len(the_initial_vars)):\n",
    "    train_res=run_optimization(data_tth, features_set, oddevensplit=True, fillna=False, debug=False)\n",
    "    auc_per_var.append([train_res['auc_test'], deepcopy(features_set), train_res])\n",
    "    print('Trained on', features_set) \n",
    "    print('or ', train_res['features'])\n",
    "    print('Importance', train_res['sorted_importance'])\n",
    "    print('removing ', min(train_res['sorted_importance'].items(), key=lambda x: x[1])[0])\n",
    "    features_set.remove(min(train_res['sorted_importance'].items(), key=lambda x: x[1])[0])\n",
    "    print('Next time on', features_set)  \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(auc_per_var))], [i[0] for i in auc_per_var])\n",
    "plt.xlabel('Removing each time the least important variable')\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "# Access best model\n",
    "best_model_set= max(auc_per_var,key=lambda item:item[0])[2]\n",
    "\n",
    "print(best_model_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "skjdfhs\n"
     ]
    }
   ],
   "source": [
    "d = [ \n",
    "    ['chimi', 3],\n",
    "    ['skjdfhs', 123],\n",
    "    ['as', 2],\n",
    "    ['chip', 33]\n",
    "]\n",
    "\n",
    "print( max(x[1] for x in d) )\n",
    "print( max(d,key=lambda item:item[1])[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "\n",
    "convert_model(model,input_variables=input_vars,output_xml=os.path.join(save_dir,'xgboost-{}.xml'.format(channel)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
