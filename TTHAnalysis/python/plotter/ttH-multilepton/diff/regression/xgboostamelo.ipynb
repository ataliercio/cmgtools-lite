{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n",
      "You are running on  wudangshan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from train_utils import *\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "import uproot\n",
    "from root_pandas import read_root\n",
    "\n",
    "\n",
    "import socket\n",
    "my_hostname=socket.gethostbyaddr(socket.gethostname())[0]\n",
    "print('You are running on ', my_hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "def load_data(debug=False):\n",
    "    idir = '{}'.format('/nfs/user/pvischia/tth/dnn/' if 'cism.ucl.be' in my_hostname else './')\n",
    "    file_tth = \"{}/tree_2lss1tau.root\".format(idir)\n",
    "    key = \"Friends\"\n",
    "    data_tth_orig = read_root(file_tth, key)\n",
    "    if debug:\n",
    "        data_tth_orig.describe()\n",
    "    return data_tth_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-jag the data with respect to the jets\n",
    "\n",
    "def dejag_data(data, debug=False):\n",
    "    test=data['SelJet_pt']\n",
    "    l = [len(i) for i in test]\n",
    "    maxl = max(l)\n",
    "\n",
    "    for lab in ['pt', 'eta', 'phi', 'mass', 'isBtag', 'isFromHadTop', 'btagDeepFlavB']:\n",
    "        label='SelJet_%s'%lab\n",
    "        tempLab=data[label]\n",
    "        data.drop([label], axis=1, inplace=True)\n",
    "        out = pd.DataFrame(tempLab.tolist(),columns=[ 'SelJet%s_%s'%(i, lab) for i in range(maxl)])\n",
    "        data = pd.concat([data, out], axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ill-defined mass columns\n",
    "def remove_masses(data, debug=False):\n",
    "    for lab in data.columns:\n",
    "        if 'mass' in lab:\n",
    "            data.drop([lab], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_names(useJets=0, debug=False):\n",
    "    thevars=[\n",
    "            'Lep1_pt', \n",
    "            'Lep2_pt', 'Lep1_eta', 'Lep2_eta', 'Lep1_phi', 'Lep2_phi',\n",
    "             'nSelJets',\n",
    "             'met', 'met_phi', \n",
    "             'HTT_score', \n",
    "             'Hj_tagger_hadTop',\n",
    "             'mindr_lep2_jet', 'mindr_lep1_jet', 'avg_dr_jet',\n",
    "             'dPhiLL_BBframe_2lss', 'dEtaLL_BBframe_2lss', 'dPhiBB_LLframe_2lss',\n",
    "             'dEtaBB_LLframe_2lss',\n",
    "             'dEtaBB_2lss',\n",
    "             'mTTH_2lss1tau',\n",
    "             'theta_higgs_ttbar_TTHsystem_2lss1tau',\n",
    "             'thetaTopTop_ttbarframe_2lss1tau', \n",
    "             'Tau_pt', 'Tau_eta', 'Tau_phi'\n",
    "            ]\n",
    "    if useJets>0:\n",
    "        for i in range(useJets):\n",
    "            thevars.append('SelJet%s_pt'%i)\n",
    "            thevars.append('SelJet%s_eta'%i)\n",
    "            thevars.append('SelJet%s_phi'%i)\n",
    "            #thevars.append('SelJet%s_mass'%i)\n",
    "            thevars.append('SelJet%s_isBtag'%i)\n",
    "            thevars.append('SelJet%s_isFromHadTop'%i)\n",
    "            thevars.append('SelJet%s_btagDeepFlavB'%i)\n",
    "    if debug:\n",
    "        print('Training features:', thevars)\n",
    "        \n",
    "    return thevars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(data, features, oddevensplit=False, fillna=False, debug=False):\n",
    "    labs=deepcopy(features)\n",
    "    if oddevensplit:\n",
    "        labs.append('event')\n",
    "    bkg = data[labs]\n",
    "    sig = data[labs]\n",
    "\n",
    "    if fillna:\n",
    "        bkg.fillna(0, inplace=True)\n",
    "        sig.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    wgtbkg=pd.DataFrame(data['weight_SM'].tolist(), columns=['weight'])\n",
    "    wgtsig=pd.DataFrame(data['weight_CP_odd'].tolist(), columns=['weight'])\n",
    "\n",
    "    labbkg = pd.DataFrame(np.zeros_like(bkg[features[0]]).tolist(), columns=['label'])\n",
    "    labsig = pd.DataFrame(np.ones_like(bkg[features[0]]).tolist(), columns=['label'])\n",
    "\n",
    "    bkg = pd.concat([bkg, wgtbkg, labbkg], axis=1)\n",
    "    sig = pd.concat([sig, wgtsig, labsig], axis=1)\n",
    "\n",
    "    sig['weight'] = sig['weight'].apply(lambda x: x if x <10. else 10.)\n",
    "\n",
    "    labelled_set=pd.concat([sig, bkg], axis=0)\n",
    "    if False:\n",
    "        sns.pairplot(labelled_set, hue='label')\n",
    "    if oddevensplit:\n",
    "        labs.append('weight')\n",
    "        X=labelled_set[labs]\n",
    "        y=labelled_set['label']\n",
    "        weight=labelled_set['weight']\n",
    "        if debug:\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            print(bkg.head())\n",
    "            print(sig.head())\n",
    "\n",
    "        X_wgt=X['weight']\n",
    "\n",
    "        X_train=X[X['event']%2 == 0]\n",
    "        X_test =X[X['event']%2 != 0]\n",
    "        y_train=y[X['event']%2 == 0]\n",
    "        y_test =y[X['event']%2 != 0]\n",
    "        \n",
    "        X_train_wgt=X_train['weight']\n",
    "        X_train=X_train.drop(['weight'], axis=1, inplace=False)\n",
    "        X_train=X_train.drop(['event'], axis=1, inplace=False)\n",
    "\n",
    "        X_test_wgt=X_test['weight']\n",
    "        X_test=X_test.drop(['weight'], axis=1, inplace=False)\n",
    "        X_test=X_test.drop(['event'], axis=1, inplace=False)\n",
    "\n",
    "        return {'X_train'     : X_train,\n",
    "                'X_test'      : X_test, \n",
    "                'X_train_wgt' : X_train_wgt,\n",
    "                'X_test_wgt'  : X_test_wgt,\n",
    "                'y_train'     : y_train,\n",
    "                'y_test'      : y_test \n",
    "               }\n",
    "    else:\n",
    "        labelled_set=shuffle(labelled_set)\n",
    "        labs.append('weight')\n",
    "        X=labelled_set[labs]\n",
    "        y=labelled_set['label']\n",
    "        weight=labelled_set['weight']\n",
    "        if debug:\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            print(bkg.head())\n",
    "            print(sig.head())\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1534534, shuffle=True)\n",
    "\n",
    "        X_train_wgt=X_train['weight']\n",
    "        X_train=X_train.drop(['weight'], axis=1, inplace=False)\n",
    "        X_test_wgt=X_test['weight']\n",
    "        X_test=X_test.drop(['weight'], axis=1, inplace=False)\n",
    "\n",
    "        return {'X_train'     : X_train,\n",
    "                'X_test'      : X_test, \n",
    "                'X_train_wgt' : X_train_wgt,\n",
    "                'X_test_wgt'  : X_test_wgt,\n",
    "                'y_train'     : y_train,\n",
    "                'y_test'      : y_test\n",
    "               }\n",
    "        #train_dmatrix = xgb.DMatrix(data=X_train,label=y_train,weight=X_train_wgt)\n",
    "        #test_dmatrix = xgb.DMatrix(data=X_test,label=y_test,weight=X_test_wgt)\n",
    "\n",
    "    #perm = np.random.permutation(len(X_train))\n",
    "    #X_train = X_train[perm]\n",
    "    #y_train = y_train[perm]\n",
    "    #X_train_wgt = X_train_wgt[perm]\n",
    "\n",
    "    #perm = np.random.permutation(len(X_train))\n",
    "    #X_test = X_test[perm]\n",
    "    #y_test = y_test[perm]\n",
    "    #X_test_wgt = X_test_wgt[perm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(X, y, wgt, features, label):\n",
    "    fix, axs= plt.subplots(10,10, figsize=(40,40))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i,ax in enumerate(axs):\n",
    "        if i >= len(features):\n",
    "            continue\n",
    "        var=features[i]\n",
    "        if var=='weight':\n",
    "            continue\n",
    "        ax=axs[i]\n",
    "        #ax.hist(X_train[var], label='%s'%var)\n",
    "        ax.hist(X[var][y[:]==0 ], weights=wgt[y[:]==0 ], density=True, alpha=0.5, bins=20)\n",
    "        ax.hist(X[var][y[:]==1 ], weights=wgt[y[:]==1 ], density=True, alpha=0.5, bins=20)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_title('%s dataset'%label)\n",
    "        ax.set_xlabel(var)\n",
    "    plt.show()\n",
    "    plt.hist(wgt[y[:]==0 ], alpha=0.5, bins=np.linspace(0,20,200))\n",
    "    plt.yscale(\"log\")\n",
    "    plt.hist(wgt[y[:]==1 ], alpha=0.5, bins=np.linspace(0,20,200))\n",
    "    plt.title('%s dataset')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalized=True, cmap='bone'):\n",
    "    plt.figure(figsize=[7, 6])\n",
    "    norm_cm = cm\n",
    "    if normalized:\n",
    "        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "debug=False\n",
    "data_tth_orig = load_data(debug=debug)\n",
    "data_tth=data_tth_orig # save the original\n",
    "data_tth=dejag_data(data_tth, debug=debug)\n",
    "data_tth=remove_masses(data_tth, debug=debug)\n",
    "    \n",
    "the_initial_vars=load_feature_names(useJets=0, debug=debug) # add \"useJets\" quadrimomenta of jets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single model (the basic template)\n",
    "\n",
    "def train_single_model(params, data, features, oddevensplit=True, fillna=False, debug=False):\n",
    "    # Start with a first thing with all variables\n",
    "    datasets= get_train_and_test(data, features, oddevensplit=oddevensplit, fillna=fillna, debug=debug)\n",
    "    X_train     = datasets['X_train']\n",
    "    X_test      = datasets['X_test']\n",
    "    X_train_wgt = datasets['X_train_wgt']\n",
    "    X_test_wgt  = datasets['X_test_wgt']\n",
    "    y_train     = datasets['y_train']\n",
    "    y_test      = datasets['y_test']\n",
    "\n",
    "    if debug:\n",
    "        plot_features(X_train, y_train, X_train_wgt, the_initial_vars, 'training')\n",
    "        plot_features(X_test , y_test , X_test_wgt , the_initial_vars, 'test')\n",
    "    # WORKING PROTOTYPE: xg_class = xgb.XGBClassifier(n_estimators=120, max_depth=3, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, gamma=1)\n",
    "    # Theoretically it should just accept \"params=params\"\n",
    "    xg_class = xgb.XGBClassifier(n_estimators=params['n_estimators'], \n",
    "                                 max_depth=params['max_depth'],\n",
    "                                 learning_rate=params['learning_rate'],\n",
    "                                 subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],\n",
    "                                 gamma=params['gamma'])\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xg_class.fit(X=X_train,y=y_train,eval_metric=[\"error\", \"logloss\"], early_stopping_rounds=10, eval_set=eval_set,sample_weight=X_train_wgt,sample_weight_eval_set=[X_train_wgt, X_test_wgt])\n",
    "\n",
    "    preds_train = xg_class.predict_proba(X_train)[:,0]\n",
    "    preds_test = xg_class.predict_proba(X_test)[:,0]\n",
    "\n",
    "    #preds_train=np.argmax(xg_class.predict_proba(X_train), axis=1)\n",
    "    #preds_test =np.argmax(xg_class.predict_proba(X_test), axis=1)\n",
    "\n",
    "    preds_cat_train = xg_class.predict(X_train)\n",
    "    preds_cat_test  = xg_class.predict(X_test)\n",
    "    # ROC curves\n",
    "    auc_train=plot_roc(y_train, preds_train, sample_weight=X_train_wgt, label='training', plot=False, debug=False)\n",
    "    auc_test=plot_roc(y_test, preds_test, sample_weight=X_test_wgt, label='test', plot=False, debug=False)\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, preds_cat_train,sample_weight=X_train_wgt)\n",
    "    print(\"Accuracy (train): %.2f%%\" % (accuracy_train * 100.0))\n",
    "    accuracy_test = accuracy_score(y_test, preds_cat_test, sample_weight=X_test_wgt)\n",
    "    print(\"Accuracy (test): %.2f%%\" % (accuracy_test * 100.0))\n",
    "\n",
    "    if debug:\n",
    "        xgb.plot_importance(xg_class)\n",
    "        plt.figure(figsize = (16, 12))\n",
    "        plt.show()        \n",
    "        results = xg_class.evals_result()\n",
    "        epochs = len(results['validation_0']['logloss'])\n",
    "        x_axis = range(0, epochs)\n",
    "        # plot log loss\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.title('XGBoost Log Loss')\n",
    "        plt.show()\n",
    "        # plot classification error\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "        ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "        ax.legend()\n",
    "        plt.ylabel('Classification Error')\n",
    "        plt.title('XGBoost Classification Error')\n",
    "        plt.show()\n",
    "\n",
    "        cm = confusion_matrix(y_test, preds_cat_test, sample_weight=X_test_wgt)\n",
    "        plot_confusion_matrix(cm, ['CP odd', 'CP even'])\n",
    "        plot_rel_pred(y_test, preds_test, 'test')\n",
    "        plot_pred(y_test, preds_test, 'test')\n",
    "        plot_weights(X_test_wgt, y_test, 'test')\n",
    "        \n",
    "    return {'auc_train': auc_train, 'auc_test': auc_test, 'model': xg_class, 'features': features, 'sorted_importance': dict(sorted(xg_class.get_booster().get_score(importance_type='weight').items(), key=lambda item: item[1])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED (it claims XGBClassifier doesn't implement the fit() method, which is manifestly untrue)\n",
    "def run_optimization_randomizedsearch(data, features, oddevensplit=True, fillna=False, debug=False):\n",
    "    # Here must pass the parameters dictionary params, etc\n",
    "    # (must change train_single_model to accept the dictionary of parameters,\n",
    "    # and then set up the full grid scan)\n",
    "    #train_res= train_single_model(data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "\n",
    "    datasets= get_train_and_test(data, features, oddevensplit=oddevensplit, fillna=fillna, debug=debug)\n",
    "    X_train     = datasets['X_train']\n",
    "    X_test      = datasets['X_test']\n",
    "    X_train_wgt = datasets['X_train_wgt']\n",
    "    X_test_wgt  = datasets['X_test_wgt']\n",
    "    y_train     = datasets['y_train']\n",
    "    y_test      = datasets['y_test']\n",
    "\n",
    "    grid={'learning_rate' : [0.01, 0.02, 0.05, 0.1],\n",
    "          'n_estimators' : [50, 100, 200, 500, 1000],\n",
    "          'max_depth' : [3, 4, 5, 8],\n",
    "          'subsample' : [0.5, 0.8, 0.9, 1.0],\n",
    "          'colsample_bytree' : [0.3, 0.5, 0.8, 1.0],\n",
    "          'gamma' : [0, 1, 5]\n",
    "         }\n",
    "    list(ParameterGrid(grid))   \n",
    "    xg_class = xgb.XGBClassifier(n_estimators=params['n_estimators'], \n",
    "                                 max_depth=params['max_depth'],\n",
    "                                 learning_rate=params['learning_rate'],\n",
    "                                 subsample=params['subsample'],\n",
    "                                 colsample_bytree=params['colsample_bytree'],\n",
    "                                 gamma=params['gamma'])\n",
    "    \n",
    "    folds = 3\n",
    "    param_comb = 5\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "    random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    random_search.fit(X=X_train,y=y_train,eval_metric=[\"error\", \"logloss\"], early_stopping_rounds=10, eval_set=eval_set,sample_weight=X_train_wgt,sample_weight_eval_set=[X_train_wgt, X_test_wgt])\n",
    "    print('\\n Best estimator:')\n",
    "    print(random_search.best_estimator_)\n",
    "    print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "    print(random_search.best_score_ * 2 - 1)\n",
    "    print('\\n Best hyperparameters:')\n",
    "    print(random_search.best_params_)\n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full hyperparameters optimization for a single set of variables\n",
    "\n",
    "# I know I should use e.g. Parzen tree estimators from hyperopt as I did for the charged Higgs with 2016 dataset,\n",
    "# but I feel lazy and the hyperparameter space I want to check is actually not that large,\n",
    "# so I am implementing a simple grid search\n",
    "\n",
    "def run_optimization(data, features, oddevensplit=True, fillna=False, debug=False):\n",
    "    # Here must pass the parameters dictionary params, etc\n",
    "    # (must change train_single_model to accept the dictionary of parameters,\n",
    "    # and then set up the full grid scan)\n",
    "    #train_res= train_single_model(data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "\n",
    "    grid={'learning_rate' : [0.01, 0.02, 0.05, 0.1],\n",
    "          'n_estimators' : [50, 100, 200, 500, 1000],\n",
    "          'max_depth' : [3, 4, 5, 8],\n",
    "          'subsample' : [0.5, 0.8, 0.9, 1.0],\n",
    "          'colsample_bytree' : [0.3, 0.5, 0.8, 1.0],\n",
    "          'gamma' : [0, 1, 5]\n",
    "         }\n",
    "       \n",
    "    listgrid=list(ParameterGrid(grid))   \n",
    "    gridresults=[]\n",
    "    for params in listgrid:\n",
    "        train_res= train_single_model(params, data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "        gridresults.append([train_res, train_res['auc_test']])\n",
    "        print('MAXIMUM HAS NOW AUC', max(gridresults,key=lambda item:item[1])[1])\n",
    "        print('FULL LIST IS', gridresults)\n",
    "    return(max(gridresults,key=lambda item:item[1])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a single model\n",
    "if False:\n",
    "    params={'learning_rate' : 0.1,\n",
    "          'n_estimators' : 120,\n",
    "          'max_depth' : 4,\n",
    "          'subsample' : 0.8,\n",
    "          'colsample_bytree' : 1,\n",
    "          'gamma' : 1\n",
    "         }\n",
    "    train_res= train_single_model(params, data_tth, the_initial_vars, oddevensplit=True, fillna=False, debug=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# Full  optimization, where \"best\" is w.r.t. the AUC metric:\n",
    "# 1. start with a variable set\n",
    "# 2. find the model with the best hyperparameter values for that set of variables\n",
    "# 3. remove the variable that is the least important in this model\n",
    "# 4. return to point 2\n",
    "# This ends when removing variables doesnt' improve the AUC anymore\n",
    "\n",
    "\n",
    "# So far for each variables set I run a single model.\n",
    "# When I'll have implemented run_optimization (for a given features set) I'll substitute a call to it here\n",
    "# in place of the call to train_single_model\n",
    "auc_per_var=[]\n",
    "features_set=deepcopy(the_initial_vars)\n",
    "for i in range(len(the_initial_vars)):\n",
    "    train_res=run_optimization(data_tth, features_set, oddevensplit=True, fillna=False, debug=False)\n",
    "    auc_per_var.append([train_res['auc_test'], deepcopy(features_set), train_res])\n",
    "    print('Trained on', features_set) \n",
    "    print('or ', train_res['features'])\n",
    "    print('Importance', train_res['sorted_importance'])\n",
    "    print('removing ', min(train_res['sorted_importance'].items(), key=lambda x: x[1])[0])\n",
    "    features_set.remove(min(train_res['sorted_importance'].items(), key=lambda x: x[1])[0])\n",
    "    print('Next time on', features_set)  \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'AUC')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(len(auc_per_var))], [i[0] for i in auc_per_var])\n",
    "plt.xlabel('Removing each time the least important variable')\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "# Access best model\n",
    "best_model_set= max(auc_per_var,key=lambda item:item[0])[2]\n",
    "\n",
    "print(best_model_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "skjdfhs\n"
     ]
    }
   ],
   "source": [
    "d = [ \n",
    "    ['chimi', 3],\n",
    "    ['skjdfhs', 123],\n",
    "    ['as', 2],\n",
    "    ['chip', 33]\n",
    "]\n",
    "\n",
    "print( max(x[1] for x in d) )\n",
    "print( max(d,key=lambda item:item[1])[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "\n",
    "convert_model(model,input_variables=input_vars,output_xml=os.path.join(save_dir,'xgboost-{}.xml'.format(channel)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
